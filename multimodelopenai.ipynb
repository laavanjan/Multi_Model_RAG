{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2da60e",
   "metadata": {},
   "source": [
    "### Multimodal RAG (PDF With Images)\n",
    "\n",
    "![image.png](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86842228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Datascience_projects\\Multi_Model_RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain_core.documents import Document\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clip model\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "##set up the environment variables\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "## initialize the Clip model for unified embedding\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac7e35",
   "metadata": {},
   "source": [
    "### clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "This loads the pre-trained CLIP model (Vision Transformer base, patch size 32) from Hugging Face. The model can generate embeddings for both images and text, enabling comparison between them in a shared embedding space.\n",
    "\n",
    "### clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "This loads the corresponding processor, which handles preprocessing (tokenization for text, resizing/normalization for images) so that inputs are compatible with the CLIP model.\n",
    "Together, these are used for unified multimodal (text and image) embedding and retrieval tasks.\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/clip?usage=Pipeline#transformers.CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af793994",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embedding function\n",
    "def embed_image(image_data):\n",
    "    \"\"\"Embed Image Using Clip\"\"\"\n",
    "    if isinstance(image_data,str): #if path\n",
    "        image = Image.open(image_data).convert(\"RGB\")\n",
    "    else: #If PIL Image\n",
    "        image = image_data\n",
    "    \n",
    "    clip_processor(images=image,return_tensors='pt')  # pt means pytorch tensors\n",
    "    #It takes the image and applies the necessary preprocessing steps \n",
    "    # (such as resizing, normalization, and conversion to tensor format) required by the CLIP model.\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        ## Normalize embeddings to unit vector\n",
    "        features = features/ features.norm(dim=-1,keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "\n",
    "#with torch.no_grad():\n",
    "# Disables gradient calculation, which is not needed for inference and saves memory/computation.\n",
    "\n",
    "# features = clip_model.get_image_features(**inputs)\n",
    "# Passes the processed image tensor to the CLIP model to get its embedding vector.\n",
    "\n",
    "# features = features / features.norm(dim=-1, keepdim=True)\n",
    "# Normalizes the embedding vector to have unit length (L2 norm = 1), which is useful for similarity comparisons.\n",
    "\n",
    "# return features.squeeze().numpy()\n",
    "# Removes any singleton dimensions from the tensor and converts it to a NumPy array for further processing or storage.\n",
    "# This sequence extracts a normalized image embedding from the CLIP model for use in retrieval or similarity tasks.\n",
    "    \n",
    "def embed_text(text):\n",
    "    \"\"\"Embed text using CLIP.\"\"\"\n",
    "    inputs = clip_processor(\n",
    "        text=text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77  # CLIP's max token length\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        # Normalize embeddings\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5218216f",
   "metadata": {},
   "source": [
    "The package fitz is the Python interface for PyMuPDF, a library used to read, manipulate, and extract information from PDF documents and other supported file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106d184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process PDF\n",
    "pdf_path=\"multimodal-sample.pdf\"\n",
    "doc=fitz.open(pdf_path)\n",
    "# Storage for all documents and embeddings\n",
    "all_docs = []\n",
    "all_embeddings = []\n",
    "image_data_store = {}  # Store actual image data for LLM\n",
    "\n",
    "# Text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca5bafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('multimodal-sample.pdf')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,page in enumerate(doc):\n",
    "    ## process text\n",
    "    text=page.get_text()\n",
    "    if text.strip():\n",
    "        ##create temporary document for splitting\n",
    "        temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
    "        text_chunks = splitter.split_documents([temp_doc])\n",
    "\n",
    "        #Embed each chunk using CLIP\n",
    "        for chunk in text_chunks:\n",
    "            embedding = embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "\n",
    "\n",
    "\n",
    "    ## process images\n",
    "    ##Three Important Actions:\n",
    "\n",
    "    ##Convert PDF image to PIL format\n",
    "    ##Store as base64 for GPT-4V (which needs base64 images)\n",
    "    ##Create CLIP embedding for retrieval\n",
    "\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref = img[0]#Gets the reference number (xref) for the image object in the PDF.\n",
    "            base_image = doc.extract_image(xref)#Extracts the image data from the PDF using the xref.\n",
    "            image_bytes = base_image[\"image\"]#Retrieves the raw image bytes from the extracted image data.\n",
    "#Raw image bytes refer to the binary data that directly represents the image file, \n",
    "# before it is decoded or processed. This data includes all the pixel information and file encoding (such as PNG or JPEG format) as stored in the PDF\n",
    "\n",
    "            # Convert to PIL Image so you can process, display, or transform the image using Python code.\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            \n",
    "            # Create unique identifier\n",
    "            image_id = f\"page_{i}_img_{img_index}\"\n",
    "            \n",
    "            # Store image as base64 for later use with GPT-4V\n",
    "            buffered = io.BytesIO()#Creates an in-memory buffer to store the image\n",
    "            pil_image.save(buffered, format=\"PNG\")#Saves the PIL image to the buffer in PNG format.\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()#Encodes the buffered image as a base64 string for use with models like GPT-4V.\n",
    "#buffered.getvalue() gets the raw bytes of the image from the buffer.\n",
    "# base64.b64encode(...) encodes those bytes into a base64 format, which is a way to represent binary data as text.\n",
    "# .decode() converts the base64 bytes into a regular string.\n",
    "            image_data_store[image_id] = img_base64#Stores the base64-encoded image string in a dictionary with a unique identifier.\n",
    "            \n",
    "            # Embed image using CLIP\n",
    "            embedding = embed_image(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "            \n",
    "            # Create document for image\n",
    "            image_doc = Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "            )\n",
    "            all_docs.append(image_doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "doc.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81854185",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25484b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee000815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unified FAISS vector store with CLIP embeddings\n",
    "embeddings_array = np.array(all_embeddings)\n",
    "embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_docs, all_embeddings)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d17d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=[(doc.page_content,emb) for doc, emb in zip(all_docs, all_embeddings)],\n",
    "    embedding=None, #We're using pre computed embeddings\n",
    "    metadatas=[doc.metadata for doc in all_docs]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPT-4 Vision model\n",
    "llm = init_chat_model(\"openai:gpt-4.1\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bbc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_multimodal(query, k=5):\n",
    "    \"\"\"Unified retrieval using CLIP embeddings for both text and images.\"\"\"\n",
    "    # Embed query using CLIP\n",
    "    query_embedding = embed_text(query)\n",
    "    \n",
    "    # Search in unified vector store\n",
    "    results = vector_store.similarity_search_by_vector(\n",
    "        embedding=query_embedding,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_message(query, retrieved_docs):\n",
    "    \"\"\"Create a message with both text and images for GPT-4V.\"\"\"\n",
    "    content = []\n",
    "    \n",
    "    # Add the query\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": f\"Question: {query}\\n\\nContext:\\n\"\n",
    "    })\n",
    "    \n",
    "    # Separate text and image documents\n",
    "    text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "    image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
    "    \n",
    "    # Add text context\n",
    "    if text_docs:\n",
    "        text_context = \"\\n\\n\".join([\n",
    "            f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
    "            for doc in text_docs\n",
    "        ])\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Text excerpts:\\n{text_context}\\n\"\n",
    "        })\n",
    "    \n",
    "    # Add images\n",
    "    for doc in image_docs:\n",
    "        image_id = doc.metadata.get(\"image_id\")\n",
    "        if image_id and image_id in image_data_store:\n",
    "            content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\\n[Image from page {doc.metadata['page']}]:\\n\"\n",
    "            })\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Add instruction\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"\\n\\nPlease answer the question based on the provided text and images.\"\n",
    "    })\n",
    "    \n",
    "    return HumanMessage(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_pdf_rag_pipeline(query):\n",
    "    \"\"\"Main pipeline for multimodal RAG.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    context_docs = retrieve_multimodal(query, k=5)\n",
    "    \n",
    "    # Create multimodal message\n",
    "    message = create_multimodal_message(query, context_docs)\n",
    "    \n",
    "    # Get response from GPT-4V\n",
    "    response = llm.invoke([message])\n",
    "    \n",
    "    # Print retrieved context info\n",
    "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
    "    for doc in context_docs:\n",
    "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        if doc_type == \"text\":\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"  - Text from page {page}: {preview}\")\n",
    "        else:\n",
    "            print(f\"  - Image from page {page}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"What does the chart on page 1 show about revenue trends?\",\n",
    "        \"Summarize the main findings from the document\",\n",
    "        \"What visual elements are present in the document?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        answer = multimodal_pdf_rag_pipeline(query)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multi_Model_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
